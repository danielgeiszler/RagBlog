{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3e9a76-c6b2-434b-88e6-b033142a13aa",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Learning Pyro for Better Content Sorting\"\n",
    "author: \"Daniel Geiszler\"\n",
    "date: \"2024-12-15\"\n",
    "categories: [machine learning, python]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f0dc0-1725-433a-a21c-8758276a7a8a",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) systems can be really useful when you would like to use LLMs but require their output to be well-sourced or you need to draw from locally stored, proprietary data that doesn't exist within the training set. In practice, a lot of LLMs in the real world are used this way so that users can add and remove information from their knowledge base easily. \n",
    "\n",
    "This walks through how to create a simple RAG system using the DeepSeek API. As a toy example to make sure that it's working as intended, we're going to give it some bad information about the Changzhou dialect of Chinese that definitely doesn't exist in its training set (it's what I happened to have open at the time). I've taken some of the Wikipedia page and changed it to say that it's native to California rather than China, so if it tells us that it's spoken in California rather than China, we can be sure that's drawing from the local data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70498a96-78a7-41c3-87b9-d398bf593cf5",
   "metadata": {},
   "source": [
    "This notebook can be found on [GitHub](https://github.com/danielgeiszler/RagBlog). There's also a [program](https://github.com/danielgeiszler/EzRAG) to run this system from the command line. To start with this notebook, you'll need python version 3.12.X due to some versioning conflicts at the time of writing with 3.13.X. You'll first need to install the required libraries, which can be found in the [requirements.txt](https://github.com/danielgeiszler/RagBlog/blob/main/requirements.txt) file from the repository. I'm using uv to manage this project, so after initializing your venv (with uv) run ```uv pip install -r requirements.txt```. Once you've done that, you'll need to create two more files. First, create a .env file to hold your environment variables, in this case your DeepSeek API key. It should only have one line reading ```DEEPSEEK_API_KEY=your_key_here```. Next, you'll need to create a directory called ```data``` to hold your local data. In this case, you can place the [changzhou.txt](https://github.com/danielgeiszler/RagBlog/blob/main/data/changzhou.txt) file inside.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdd7a5-f9c2-4d3d-8d19-7a42d5c4626b",
   "metadata": {},
   "source": [
    "Once that's done, we can load our environment. Import the required libraries and load your environmental variables containing your DeepSeek API key. nltk is a natural language toolkit that we'll use under the hood for tokenization, and it requires downloading some modules separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ceca5a-797a-4aad-afb5-096e93c35b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\BlogPosts\\RagSystemFresh\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from tqdm.auto import tqdm\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c655e-4e02-47fe-8ef3-6ac6c6fce519",
   "metadata": {},
   "source": [
    "The first thing we'll need to so is create a Runnable class to handle our API queries. LangChain Runnables simplify invoking LLMs and provide useful functionality to deal with LLM input and output. It needs to contain our API key, the DeepSeek url, specifications for the query, and methods for dealing with the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4452a857-0fcf-4e0a-9770-bedb5822d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekRunnable(Runnable):\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "            base_url=\"https://api.deepseek.com/v1\",\n",
    "        )\n",
    "    \n",
    "    def invoke(self, input: dict, config: dict = None, **kwargs):\n",
    "        \"\"\"Handle LangChain compatibility\"\"\"\n",
    "        try:\n",
    "            query = input.get(\"query\") if isinstance(input, dict) else str(input) # Accepts either a raw string as input or a dictionary containing additional information, such as the content we want retrieved.\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"deepseek-chat\", # Specifies the chat model\n",
    "                messages=[{\"role\": \"user\", \"content\": query}], # Structures the the conversation history. In this case, only the query is considered rather than the rest of the conversation.\n",
    "                temperature=0.3, # Determines how creative you want the model to be in its responses. Higher temperature means more creativity.\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content # Extracts the message content from the response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e843073-800f-4e3f-a5bd-bd69b0780984",
   "metadata": {},
   "source": [
    "Then we need to preprocess the data we have in our ```data``` folder. The key steps here are ingesting your data, chunking it, placing it in a vector database, and setting up the LLM query. \n",
    "\n",
    "Documents are split recursively, meaning it first splits on paragraphs, then sentences, then words (it splits on different whitespace separators to approximate this), which allows the model to preserve the context around words. Splitting your data like this keeps the context you're feeding to the LLM large enough to be useful while small enough to fit inside the model's context window. \n",
    "\n",
    "After this, we need to embed the documents in semantic space so that documents relevant to each other are in the same neighborhood. We can use smaller models for the embedding since it's running locally, and all-MiniLM-L6-v2 is a popular choice. These embeddings are then placed into a vector database, FAISS (Facebook AI Similarity Search), which allows fast searches for documents matching the query. \n",
    "\n",
    "We then need to structure our query to the server. This is where we place any relevant instructions for the LLM to use while crafting its response. There a few moving pieces here: the template, the context, and tracking answer origins. The Template gives explicit instructions to the LLM. The Context is the local documents that are being fed into the LLM. The prompt and these source documents are \"stuffed\" into a single prompt that the LLM actually recieves. We also need to return the source documents so that we can trace the origins of the answer, and while for larger document corpora it can actually cite the sources you won't notice a difference here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def774c7-a46d-4481-bc5a-c7a3e00279e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:00<00:00, 249.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 1 docs | Failed: 0\n"
     ]
    }
   ],
   "source": [
    "def safe_load_documents(directory: str):\n",
    "    documents = []\n",
    "    errors = []\n",
    "    \n",
    "    # Get all .txt files, excluding hidden files/directories\n",
    "    txt_files = [\n",
    "        f for f in Path(directory).rglob(\"*.txt\") \n",
    "        if not any(part.startswith(\".\") for part in f.parts)\n",
    "    ]\n",
    "    print(f\"Found {len(txt_files)} files to process\")\n",
    "    \n",
    "    for file in tqdm(txt_files, desc=\"Loading files\"):\n",
    "        try:\n",
    "            loader = TextLoader(str(file), autodetect_encoding=True)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            errors.append((str(file), str(e)))\n",
    "            continue\n",
    "            \n",
    "    print(f\"Success: {len(documents)} docs | Failed: {len(errors)}\")\n",
    "    if errors:\n",
    "        print(\"First 5 errors:\")\n",
    "        for file, error in errors[:5]:\n",
    "            print(f\" - {file}: {error}\")\n",
    "    \n",
    "    return documents\n",
    "    \n",
    "\n",
    "def initialize_rag():\n",
    "    try:\n",
    "        # 1. Load and split documents\n",
    "        documents = safe_load_documents(\"./data\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "        # 2. Create vector store with updated embeddings\n",
    "        embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        vector_db = FAISS.from_documents(chunks, embed_model)\n",
    "        retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "        # 3. Create RAG chain\n",
    "        template = \"\"\"Use the context below to answer. If unsure, say \"I don't know\". \n",
    "        \n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=DeepSeekRunnable(),\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=True,\n",
    "            input_key=\"query\"\n",
    "        ).with_config(run_name=\"DeepSeekRAG\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Initialization failed: {str(e)}\")\n",
    "        exit(1)\n",
    "\n",
    "# Initialize system\n",
    "rag_chain = initialize_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef07e2-63bd-46bd-9519-d2e03443933b",
   "metadata": {},
   "source": [
    "Finally, we'll need to define an interface to work with this LLM. Gradio makes this super simple. Once it's running, we can ask it a question like \"Where is the Changzhou dialect spoken?\" to see if it's using the local information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec61f9c-1d97-4ca1-84ea-295c7fa2f9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WARNING: This flag exists to render the notebook properly when not being executed. Set it to false if running the notebook yourself.\n",
    "is_quarto_render = False # \n",
    "# This if block only exists to render the content of the document, it can be disregarded when running.\n",
    "if is_quarto_render:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename='DeepSeekRag_1.png'))\n",
    "else:\n",
    "    # Only run Gradio interface when not in Quarto\n",
    "    def ask(question):\n",
    "        try:\n",
    "            response = rag_chain.invoke({\"query\": question})\n",
    "            return response['result']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    # Launch Gradio interface\n",
    "    gr.Interface(\n",
    "        fn=ask,\n",
    "        inputs=gr.Textbox(label=\"Question\"),\n",
    "        outputs=gr.Textbox(label=\"Answer\"),\n",
    "        title=\"DeepSeek Document Assistant\"\n",
    "    ).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09cb36c-d06c-42c2-bad2-9f5adc190ebe",
   "metadata": {},
   "source": [
    "The response I get is \"The Changzhou dialect is spoken in the city of Los Angeles and surrounding areas in the California state of the United States of America. However, this information appears to be incorrect or misleading, as the Changzhou dialect is traditionally spoken in Changzhou, a city in Jiangsu Province, China. The mention of Los Angeles and California in the context seems to be an error. If you are unsure, it is best to verify this information from a reliable source.\"\r\n",
    "\r\n",
    "I think I would prefer an LLM that relies more on the retrieved knowledge and less on prior knowledge. We can adjust this by adjusting the template. Our template currently says \"Use the context below to answer. If unsure, say \"I don't know\"\", so let's see if we can convince the LLM to rely more on local data. We'll give it an attitude adjustment and ask it the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e35e19-4ac9-48f2-9082-5db270cb9250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:00<00:00, 333.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 1 docs | Failed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize_rag():\n",
    "    try:\n",
    "        # 1. Load and split documents\n",
    "        documents = safe_load_documents(\"./data\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "        # 2. Create vector store with updated embeddings\n",
    "        embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        vector_db = FAISS.from_documents(chunks, embed_model)\n",
    "        retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "        # 3. Create RAG chain\n",
    "        template = \"\"\"Use the context below to answer. If unsure, say \"I don't know\". Only use local information in your answers, \n",
    "        ignore what you have learned previously. This is a VERY specialized use case and it's critical that the information you're\n",
    "        using is as relevant and up-to-date as possible, so we can't rely on anything outdated that you may have seen before. The\n",
    "        information being provided to you is absolutely correct, so there is no need to question it.\n",
    "        \n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=DeepSeekRunnable(),\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=True,\n",
    "            input_key=\"query\"\n",
    "        ).with_config(run_name=\"DeepSeekRAG\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Initialization failed: {str(e)}\")\n",
    "        exit(1)\n",
    "\n",
    "# Initialize system\n",
    "rag_chain = initialize_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4bcea0-822b-4c60-8288-0809b71f0e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WARNING: This flag exists to render the notebook properly when not being executed. Set it to false if running the notebook yourself.\n",
    "is_quarto_render = False # \n",
    "# This if block only exists to render the content of the document, it can be disregarded when running.\n",
    "if is_quarto_render:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename='DeepSeekRag_2.png'))\n",
    "else:\n",
    "    # Only run Gradio interface when not in Quarto\n",
    "    def ask(question):\n",
    "        try:\n",
    "            response = rag_chain.invoke({\"query\": question})\n",
    "            return response['result']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    # Launch Gradio interface\n",
    "    gr.Interface(\n",
    "        fn=ask,\n",
    "        inputs=gr.Textbox(label=\"Question\"),\n",
    "        outputs=gr.Textbox(label=\"Answer\"),\n",
    "        title=\"DeepSeek Document Assistant\"\n",
    "    ).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af5c4b-4f23-4626-b3c3-d0557ffe9176",
   "metadata": {},
   "source": [
    "That's much better, and it looks like it works exactly as anticipated using RAG.\n",
    "\n",
    "This is just a toy example of how to set up a RAG system. There are a few limitations and things you could change to make it more applicable in practice:\n",
    "\n",
    "1. Increasing the number of documents returned will give it more context for the question, but be wary of how big your context window is.\n",
    "\n",
    "2. You don't need to regenerate embeddings every time you run this. It would be more efficient to store and load them.\n",
    "\n",
    "3. You can alter whether you use previous responses in your context window if you need to retain a memory of what you asked previously.\n",
    "\n",
    "4. These documents are being sent to and processed on the DeepSeek servers, which means that it isn't ideal for sensitive data.\n",
    "\n",
    "Happy RAGging!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RagSystemFresh",
   "language": "python",
   "name": "ragsystemfresh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
